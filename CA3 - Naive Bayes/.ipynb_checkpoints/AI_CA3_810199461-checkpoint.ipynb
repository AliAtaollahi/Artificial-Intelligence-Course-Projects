{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a8f063",
   "metadata": {},
   "source": [
    "In the name of god\n",
    "\n",
    "Ali Ataollahi / 810199461\n",
    "\n",
    "AI_FALL_1401 / CA3 / Naive Bayes featureifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a33367",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d58b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in c:\\users\\ali18\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in c:\\users\\ali18\\anaconda3\\lib\\site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: six in c:\\users\\ali18\\appdata\\roaming\\python\\python39\\site-packages (from nltk==3.3->hazm) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install hazm\n",
    "!python.exe -m pip install --upgrade pip\n",
    "import pandas as pd\n",
    "from math import *\n",
    "from hazm import stopwords_list, Normalizer, word_tokenize\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b93db",
   "metadata": {},
   "source": [
    "# Part1 : preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6deb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('files/train.csv')\n",
    "data_test = pd.read_csv('files/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62961c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords_list(\"stopwords.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5885a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "def clean_data(data):\n",
    "    for i in range(len(data['content'])):\n",
    "        sentence = normalizer.normalize(data['content'][i])\n",
    "        words = word_tokenize(sentence)\n",
    "        final_words = [word.replace('\\u200c', '') for word in words]\n",
    "        final_words = [word for word in final_words if word not in stop_words]\n",
    "        data['content'][i] = final_words\n",
    "\n",
    "clean_data(data_train) \n",
    "clean_data(data_test)       \n",
    "print(data_train['content'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9a9dda",
   "metadata": {},
   "source": [
    "As you can see, first we read the information of the files, then we normalize 'content' col and do not put in output words that are similar to stopwords.\n",
    "\n",
    "#### Question 1\n",
    "stemming : stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. \n",
    "\n",
    "lemmatization : the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "diffrence :\n",
    "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "https://d2mk45aasx86xg.cloudfront.net/difference_between_Stemming_and_lemmatization_8_11zon_452539721d.webp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c5546",
   "metadata": {},
   "source": [
    "# Part 2 : problem process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(data_train['label']))\n",
    "mistakes = []\n",
    "labels_freq_count = [0 for i in range(len(labels))] \n",
    "labels_dicts = [{} for i in range(len(labels))] \n",
    "\n",
    "for i in range(data_train.shape[0]):\n",
    "    label_index = labels.index(data_train['label'][i])\n",
    "    for word in data_train['content'][i]:\n",
    "        if word in labels_dicts[label_index]:\n",
    "            labels_dicts[label_index][word] += 1\n",
    "            labels_freq_count[label_index] += 1\n",
    "        else:\n",
    "            labels_dicts[label_index][word] = 1\n",
    "            labels_freq_count[label_index] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ebbdab",
   "metadata": {},
   "source": [
    "In this part , put words in dict base of its label. So every lable has dict and in this dict there is words and their frequency.\n",
    "\n",
    "In next step we try to predict label of contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd57e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = dict(zip(labels, [0] * len(labels))) \n",
    "fp = dict(zip(labels, [0] * len(labels))) \n",
    "fn = dict(zip(labels, [0] * len(labels))) \n",
    "\n",
    "def get_feature_prob(word_list, freq_count, word):\n",
    "    if (word not in word_list):\n",
    "        return -inf\n",
    "    else:\n",
    "        return log((word_list[word]) / freq_count)\n",
    "    \n",
    "predict_label = [''] * data_test.shape[0]\n",
    "corrected = 0\n",
    "\n",
    "for i in range(data_test.shape[0]):\n",
    "    labels_prob = dict(zip(labels, [0] * len(labels))) \n",
    "    for word in data_test['content'][i]:\n",
    "        for label in labels_prob:\n",
    "            label_index = labels.index(label)\n",
    "            labels_prob[label] += get_feature_prob(labels_dicts[label_index], labels_freq_count[label_index], word)\n",
    "    \n",
    "    predict_label = max(labels_prob, key=labels_prob.get)\n",
    "    \n",
    "    label = data_test['label'][i]\n",
    "    if predict_label == data_test['label'][i]:\n",
    "        corrected += 1\n",
    "        tp[label] += 1\n",
    "    else :\n",
    "        fn[label] += 1\n",
    "        fp[predict_label] += 1\n",
    "\n",
    "print(\"the accuracy without smoothing method :\", corrected / data_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291a9ef",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "$$ P(c|x) = \\frac{P(x|c)P(c)}{P(x)} $$\n",
    "\n",
    "* Evidence : the probability of the $x$ used to update the prior. \n",
    "\n",
    "In our case, the probability of the occurness of the word $x$ in the given text. computed by dividing the $x$ freq by the sum of the freq of all the text words.W e can ignore it because of we do not need to compute this probability because we want to compare two classes and this parameter is same for all classes.\n",
    "\n",
    "* Likelihood : if an event $c$ has already occurred, the probability that the event $x$ will occur given the knowledge. \n",
    "\n",
    "In our case, likelihood is the probability of the occurness of the word $x$ given the class $c$ (from 6 classes). computed by dividing the $x$ freq in the class $c$ by the sum of freq of all the words in the class $c$. \n",
    "\n",
    "* Prior : the probability of the the class $c$. \n",
    "\n",
    "In our case is In training data is symmetric. Therefore, we do not need to compute it (in all labels is same). But in test data is not symmetric\n",
    "\n",
    "* Posterior : if an event $x$ has already occurred, the probability that the event $c$ will occur given the knowledge. \n",
    "\n",
    "In our case, posterior is the probability of the occurness of the class $c$ given the word $x$. computed by the given formula. We will compare this probability for 6 classes to choose one of the classes for the given content. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b9e55",
   "metadata": {},
   "source": [
    "#### Bigrams\n",
    "\n",
    "#### Question 3\n",
    "example:\n",
    "\n",
    "در مراسم تدفین شرکت میکنم\n",
    "\n",
    "در فلان شرکت سرمایه گذاری میکنم\n",
    "\n",
    "It helps to predict much better. Because we save more structure of that type of sentence and meaning of that and consider place of word in sentence. So bigrams increase accuracy. \n",
    "\n",
    "n-bigrams can be better or can't. This is related to n and the sentences we predict. but generally if n is not too big, the result of predict is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0c5a9",
   "metadata": {},
   "source": [
    "### Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86659651",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_with_additive_smoothing = dict(zip(labels, [0] * len(labels))) \n",
    "fp_with_additive_smoothing = dict(zip(labels, [0] * len(labels))) \n",
    "fn_with_additive_smoothing = dict(zip(labels, [0] * len(labels))) \n",
    "\n",
    "ZERO = 1/9999999999999999999999999999999999999\n",
    "\n",
    "def get_feature_prob(word_list, freq_count, word):\n",
    "    if (word not in word_list):\n",
    "        return log(1 + ZERO)\n",
    "    else:\n",
    "        return log(1 + (word_list[word]) / freq_count)\n",
    "    \n",
    "predict_label = [''] * data_test.shape[0]\n",
    "corrected = 0\n",
    "\n",
    "for i in range(data_test.shape[0]):\n",
    "    labels_prob = dict(zip(labels, [0] * len(labels))) \n",
    "    for word in data_test['content'][i]:\n",
    "        for label in labels_prob:\n",
    "            label_index = labels.index(label)\n",
    "            labels_prob[label] += get_feature_prob(labels_dicts[label_index], labels_freq_count[label_index], word)\n",
    "    \n",
    "    predict_label = max(labels_prob, key=labels_prob.get)\n",
    "    \n",
    "    label = data_test['label'][i]\n",
    "    if predict_label == data_test['label'][i]:\n",
    "        corrected += 1\n",
    "        tp_with_additive_smoothing[label] += 1\n",
    "    else :\n",
    "        mistakes.append({'label':label, 'predict_label':predict_label, 'sentece': \" \".join(data_test['content'][i])})\n",
    "        fn_with_additive_smoothing[label] += 1\n",
    "        fp_with_additive_smoothing[predict_label] += 1\n",
    "\n",
    "print(\"the accuracy with smoothing method :\", corrected / data_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa784f6",
   "metadata": {},
   "source": [
    "We change the computation of class probablity by adding 1 to log and delete -inf for zero. Because of this we have better prediction and if a word is not in content, we handle and balance it better and put not pure -inf in computation.\n",
    "\n",
    "#### Question 4\n",
    "\n",
    "There is word or words contained only one type or some type of classes, Naive Bayes chooses that label as the answer which might be wrong in some of the cases.\n",
    "\n",
    "$P(x|c) = 0$ means word $x$ hasn't appeared in label $c$ .\n",
    "\n",
    "because of choosing the label which maximizes the sum $ sum_{i=1}^{n} log(P(x_i | c)) $ , and if $log(0) = - \\infty$ some label (might the true label) never will be chosen as the label\n",
    "\n",
    "#### Question 5\n",
    "\n",
    "is a technique used to smooth categorical data. Given a set of observation counts $ x   =   ⟨ x 1 , x 2 , … , x d ⟩ $ from a d -dimensional multinomial distribution with N trials, a \"smoothed\" version of the counts gives the estimator\n",
    "\n",
    "Here's how we calculate probability:\n",
    "\n",
    "$$ P(Word|Class) = \\frac{Word Count + \\alpha}{Total Words} $$\n",
    "\n",
    "How it is help ?\n",
    "\n",
    "As mentioned in Question 4, alpha will avoid the $- \\infty$ problem mentioned in since the fraction will never be equal to 0 because of the positive $\\alpha$ in the numerator.\n",
    "\n",
    "In next step we draw some plots of most appeared words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a09e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    temp_dict = dict(sorted(labels_dicts[i].items(), key = itemgetter(1), reverse = True)[:len(labels)])\n",
    "    keys = []\n",
    "    for key in temp_dict.keys():\n",
    "        keys.append(get_display(arabic_reshaper.reshape(key)))\n",
    "    values = temp_dict.values()\n",
    "    plt.bar(keys, values)\n",
    "    plt.xlabel(get_display(arabic_reshaper.reshape(labels[i])))\n",
    "    plt.ylabel('count')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02dcc25",
   "metadata": {},
   "source": [
    "# Part 3 : evaluation\n",
    "\n",
    "We write again formulas:\n",
    "\n",
    "$$accuracy = \\frac{TP + TN}{TP + FN + TN + FP}$$\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$F1 = \\frac{2 \\times precision \\times recall}{precision + recall}$$\n",
    "\n",
    "$$TP = true positive, TN = true negative, FP = false positive, FN = false negative$$\n",
    "\n",
    "**Note** : accuracy already computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bfda7",
   "metadata": {},
   "source": [
    "#### Qestion 7\n",
    "\n",
    "The recall value for will be 100% if the model always labels every data a specific class(like sport) and this is bad. So recall is not enough for evaluating.\n",
    "\n",
    "Consider a model that wrongly predict (except one content that is really recommended) all the contetns like sports class. Therefore the precision of this model will be 1.00 and this bad. So precision alone is not enough to evaluate a model.\n",
    "\n",
    "#### Qestion 8\n",
    "\n",
    "The F1 score is the harmonic mean of precision but the recall taking both into account. because of harmonic mean punishes extreme values precision uses the harmonic mean instead of a simple average. Therefore A classifier that has a precision of 1.0 and a recall of 0.0 has simple average = 0.5 but F1 = 0.\n",
    "\n",
    "#### Qestion 9\n",
    "\n",
    "* Micro computes these : F1 by value total tp, fn, fp (the value of the prediction for each label in the dataset is not matter)\n",
    "\n",
    "* Macro computes these : F1 for each label and returns the average (the proportion for each label in the dataset is not matter).\n",
    "\n",
    "* Weighted computes these : like macro F1 for each label and returns the average (but the proportion for each label in the dataset is not matter).\n",
    "\n",
    "without additive smoothing\n",
    "\n",
    "---\n",
    "\n",
    "$$Macro average = \\frac{1.52}{6} = 0.25$$\n",
    "\n",
    "---\n",
    "\n",
    "$$Micro average = 2 \\times \\frac{0.27 \\times 0.27}{0.27 + 0.27} = 0.27$$\n",
    "\n",
    "---\n",
    "\n",
    "$$Weighted average = \\frac{472286}{1820817} = 0.26$$\n",
    "\n",
    "---\n",
    "\n",
    "with additive smoothing\n",
    "\n",
    "---\n",
    "\n",
    "$$Macro average = \\frac{5.32}{6} = 0.89$$\n",
    "\n",
    "---\n",
    "\n",
    "$$Micro average = \\frac{2\\times 0.89 \\times 0.89}{0.89 + 0.89} = 0.89$$\n",
    "\n",
    "---\n",
    "\n",
    "$$Weighted average = \\frac{1612952}{1820817} = 0.88$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Qestion 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18da48",
   "metadata": {},
   "source": [
    "#### For without additive smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_sum, weighted_sum, overall_size, precision_numerator, precision_denominator, recall_denominator = 0, 0, 0, 0, 0, 0\n",
    "for label in labels:\n",
    "    recall = tp[label] / (tp[label] + fn[label])\n",
    "    precision = tp[label] / (tp[label] + fp[label])\n",
    "    F1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(\"label :\", label)\n",
    "    print(\"TP (true positive) :\", tp[label])\n",
    "    print(\"FP (false positive) :\", fp[label])\n",
    "    print(\"FN (false negative) :\", fn[label])\n",
    "    print(\"Precision :\", precision)\n",
    "    print(\"Recall :\", recall)\n",
    "    print(\"F1 :\", F1)\n",
    "    print()\n",
    "    label_index = labels.index(label)\n",
    "    weighted_sum += F1 * labels_freq_count[label_index]\n",
    "    overall_size += labels_freq_count[label_index]\n",
    "    precision_numerator += tp[label]\n",
    "    precision_denominator += tp[label] + fp[label]\n",
    "    recall_denominator += tp[label] + fn[label]\n",
    "    F1_sum += F1\n",
    "prec = precision_numerator / precision_denominator\n",
    "reca = precision_numerator / recall_denominator\n",
    "print(\"Macro average:\", F1_sum / len(labels))\n",
    "print(\"Micro average:\", 2 * prec * reca / (prec + reca))\n",
    "print(\"Weighted average:\", weighted_sum / overall_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491c749",
   "metadata": {},
   "source": [
    "#### For with additive smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fde3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_sum, weighted_sum, overall_size, precision_numerator, precision_denominator, recall_denominator = 0, 0, 0, 0, 0, 0\n",
    "for label in labels:\n",
    "    recall = tp_with_additive_smoothing[label] / (tp_with_additive_smoothing[label] + fn_with_additive_smoothing[label])\n",
    "    precision = tp_with_additive_smoothing[label] / (tp_with_additive_smoothing[label] + fp_with_additive_smoothing[label])\n",
    "    F1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(\"label :\", label)\n",
    "    print(\"TP (true positive) :\", tp_with_additive_smoothing[label])\n",
    "    print(\"FP (false positive) :\", fp_with_additive_smoothing[label])\n",
    "    print(\"FN (false negative) :\", fn_with_additive_smoothing[label])\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", F1)\n",
    "    print()\n",
    "    label_index = labels.index(label)\n",
    "    weighted_sum += F1 * labels_freq_count[label_index]\n",
    "    overall_size += labels_freq_count[label_index]\n",
    "    precision_numerator += tp_with_additive_smoothing[label]\n",
    "    precision_denominator += tp_with_additive_smoothing[label] + fp_with_additive_smoothing[label]\n",
    "    recall_denominator += tp_with_additive_smoothing[label] + fn_with_additive_smoothing[label]\n",
    "    F1_sum += F1\n",
    "prec = precision_numerator / precision_denominator\n",
    "reca = precision_numerator / recall_denominator\n",
    "print(\"Macro average:\", F1_sum / len(labels))\n",
    "print(\"Micro average:\", 2 * prec * reca / (prec + reca))\n",
    "print(\"Weighted average:\", weighted_sum / overall_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac743d8a",
   "metadata": {},
   "source": [
    "\n",
    "#### Qestion 11\n",
    "\n",
    "The overall performance is better when we use additive smoothing.\n",
    "\n",
    "#### Qestion 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac1777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in mistakes[:5]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff90b5",
   "metadata": {},
   "source": [
    "Reason :\n",
    "\n",
    "Some words of sentece very likely to be in another label. like \n",
    "\n",
    "بازی\n",
    "\n",
    "in first sentence can be in sport label. but this sentence is in \n",
    "\n",
    "هنری\n",
    "\n",
    "label. another example is sentence 5. there is two words \n",
    "\n",
    "مربیان , فوتبال\n",
    "\n",
    "that cause of predict of this sentence is sport"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "58712966323307a1839e3de0f8fdda70b5859dc8df68da156fa57b5e32d288b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
